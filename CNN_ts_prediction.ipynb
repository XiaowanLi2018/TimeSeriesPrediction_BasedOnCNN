{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr4 = pd.read_csv('../TEST/synthetic_rnn/data_level4.csv')\n",
    "df_week = pd.read_csv('../TEST/synthetic_rnn/dim_week.csv')\n",
    "df_lc = pd.read_csv('../TEST/synthetic_rnn/dim_product_lc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr4 = pd.merge(df_sr4,df_week,on=['week_begin','week_end','week_of_year','week_from_cny',\n",
    "                                     'dayOff'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr4 = pd.merge(df_sr4,df_lc,on=['LC','product','sub_category','category','temp','off_on_ratio'],how='inner',suffixes='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df_sr4.week_begin = pd.to_datetime(df_sr4.week_begin)\n",
    "df_sr4.week_end = pd.to_datetime(df_sr4.week_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_COL = ['week_from_cny',\n",
    " 'off_on_ratio','week_of_year',\n",
    " 'holiday_effect_noncny',\n",
    " 'is_cny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_FEATURE = [ 'sr','same_product_event_sr',\n",
    " 'same_subcategory_event_sr',\n",
    " 'same_category_event_sr',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = SR_FEATURE+ENC_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC</th>\n",
       "      <th>product</th>\n",
       "      <th>temp</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>off_on_ratio</th>\n",
       "      <th>week_begin</th>\n",
       "      <th>week_end</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>week_from_cny</th>\n",
       "      <th>dayOff</th>\n",
       "      <th>week_ind</th>\n",
       "      <th>sr</th>\n",
       "      <th>holiday_effect_noncny</th>\n",
       "      <th>with_mobile_coupon</th>\n",
       "      <th>same_product_event_sr</th>\n",
       "      <th>same_subcategory_event_sr</th>\n",
       "      <th>same_category_event_sr</th>\n",
       "      <th>is_cny</th>\n",
       "      <th>sr_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hot</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>30.082227</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.919103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hot</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.812660</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>7.919103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hot</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29.237116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.057621</td>\n",
       "      <td>0</td>\n",
       "      <td>7.919103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hot</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>2015-01-26</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>28.814277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.057621</td>\n",
       "      <td>0</td>\n",
       "      <td>7.919103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hot</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.974063</td>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>2015-02-08</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>28.387729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.057621</td>\n",
       "      <td>0</td>\n",
       "      <td>7.919103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LC  product temp  category  sub_category  off_on_ratio week_begin  \\\n",
       "0   0        0  hot         3            10      0.974063 2015-01-05   \n",
       "1   0        0  hot         3            10      0.974063 2015-01-12   \n",
       "2   0        0  hot         3            10      0.974063 2015-01-19   \n",
       "3   0        0  hot         3            10      0.974063 2015-01-26   \n",
       "4   0        0  hot         3            10      0.974063 2015-02-02   \n",
       "\n",
       "    week_end  week_of_year  week_from_cny  dayOff  week_ind         sr  \\\n",
       "0 2015-01-11             2              0       2         0  30.082227   \n",
       "1 2015-01-18             3              0       2         1  29.812660   \n",
       "2 2015-01-25             4              0       2         2  29.237116   \n",
       "3 2015-02-01             5              1       2         3  28.814277   \n",
       "4 2015-02-08             6              2       2         4  28.387729   \n",
       "\n",
       "   holiday_effect_noncny  with_mobile_coupon  same_product_event_sr  \\\n",
       "0                    1.0                 0.0                    0.0   \n",
       "1                    1.0                 0.0                    0.0   \n",
       "2                    1.0                 0.0                    0.0   \n",
       "3                    1.0                 0.0                    0.0   \n",
       "4                    1.0                 0.0                    0.0   \n",
       "\n",
       "   same_subcategory_event_sr  same_category_event_sr  is_cny  sr_level  \n",
       "0                        0.0                0.000000       0  7.919103  \n",
       "1                        0.0                0.000000       0  7.919103  \n",
       "2                        0.0               38.057621       0  7.919103  \n",
       "3                        0.0               38.057621       0  7.919103  \n",
       "4                        0.0               38.057621       0  7.919103  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sr4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df_all,input_sequence_length,pred_length,week_length=156,lag=1): \n",
    "    #### Input data formatting   \n",
    "    date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df_all.week_begin[0:week_length]]),\n",
    "                              data=[i for i in range(len(df_all.week_begin[0:week_length]))])\n",
    "    train_last_week_begin = date_to_index.index[0]\n",
    "    test_last_week_begin = week_length - pred_length - input_sequence_length - lag\n",
    "\n",
    "    val_last_week_end = week_length - pred_length - lag\n",
    "    val_last_week_begin = val_last_week_end - pred_length - input_sequence_length -lag\n",
    "    \n",
    "    train_last_week_end = val_last_week_end - pred_length - lag\n",
    "    \n",
    "    train_time_split = date_to_index.index[train_last_week_end]\n",
    "    test_time_split = date_to_index.index[test_last_week_begin]   \n",
    "    val_time_split_begin = date_to_index.index[val_last_week_begin]   \n",
    "    val_time_split_end = date_to_index.index[val_last_week_end] \n",
    "    \n",
    "    train = df_all[df_all['week_begin']<=pd.to_datetime(train_time_split)]\n",
    "    test = df_all[df_all['week_begin']>=pd.to_datetime(test_time_split)]\n",
    "    val = df_all[(df_all['week_begin']>=pd.to_datetime(val_time_split_begin))&\n",
    "                (df_all['week_begin']<=pd.to_datetime(val_time_split_end))]\n",
    "\n",
    "    return train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_LENGTH = 18*4\n",
    "PRED_LENGTH = 18\n",
    "#fea_num = len(FEATURE_COL)\n",
    "BATCH_SIZE = 128\n",
    "whole_ts = PRED_LENGTH+ENC_LENGTH\n",
    "TRAIN,TEST,VAL = split_train_test(df_sr4,ENC_LENGTH,PRED_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of train data is from:2015-01-05 00:00:00 to:2017-04-10 00:00:00\n",
      "Period of test data is from:2016-04-04 00:00:00 to:2017-12-25 00:00:00\n",
      "Period of val data is from:2015-11-23 00:00:00 to:2017-08-21 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('Period of train data is from:{} to:{}'.format(np.min(TRAIN.week_begin),np.max(TRAIN.week_begin)))\n",
    "print('Period of test data is from:{} to:{}'.format(np.min(TEST.week_begin),np.max(TEST.week_begin)))\n",
    "print('Period of val data is from:{} to:{}'.format(np.min(VAL.week_begin),np.max(VAL.week_begin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "enc = OneHotEncoder()\n",
    "f_mtx = df_sr4[FEATURE].iloc[:,1:]\n",
    "f_in = np.array(f_mtx)[:,(len(SR_FEATURE)-1):]\n",
    "t_enc = enc.fit(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39962"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_FEATURE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_num(df_sample):\n",
    "    #std_feature = StandardScaler.fit_transform(feature_mtx)\n",
    "    enc_mtx = enc.transform(df_sample).toarray()\n",
    "    return (enc_mtx.shape[1]+len(SR_FEATURE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_info(df):\n",
    "    df_group = df.groupby(['LC','product'])\n",
    "    feature_data = []\n",
    "    sr_data = []\n",
    "    for i in df_group:\n",
    "        # print(s_data.shape)\n",
    "        s_sample = i[1].sort_values(by=['week_begin'],ascending=True)\n",
    "        row = i[1].shape[0]\n",
    "        for j in range(0,row-whole_ts,1):\n",
    "            x_col = s_sample.iloc[j:(j+whole_ts)][FEATURE]\n",
    "            x = x_col.iloc[:,1:]\n",
    "            y = x_col.iloc[:,0]\n",
    "            feature_data.append(x.values)\n",
    "            sr_data.append(y.values)\n",
    "    return feature_data,sr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FEATURE_DATA,TRAIN_SR_DATA = get_ts_info(TRAIN)\n",
    "VAL_FEATURE_DATA,VAL_SR_DATA = get_ts_info(VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_FEATURE_DATA[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_matrix(feature_list,sr_list,index,input_sequence_length,target_sequence_length):\n",
    "    ############## get time series data #################\n",
    "    whole = input_sequence_length+target_sequence_length\n",
    "    TS_mean = []\n",
    "    X = sr_list[index]\n",
    "    X = np.log1p(X)\n",
    "    ts_mean = X.mean()\n",
    "    TS_mean.append(ts_mean)\n",
    "    X = X/ts_mean\n",
    "    #X_input = X[:input_sequence_length]\n",
    "    Y = X[input_sequence_length:]\n",
    "    \n",
    "    feature = get_feature_mtx(feature_list[index],len(SR_FEATURE)-1)\n",
    "    # f = get_enc_data(feature)\n",
    "    # other_features = df[SR_FEATURE][:whole]\n",
    "    # feature = np.hstack([stable_features,other_features])\n",
    "    #Y_feature = np.hstack([df[ENC_COL][input_sequence_length:],df[SR_FEATURE][input_sequence_length:]])\n",
    "    return X,feature,Y,TS_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_mtx(x,f_start):\n",
    "    #print(x[:,f_start:].shape)\n",
    "    mtx = enc.transform(x[:,f_start:]).toarray()\n",
    "    #print(mtx.shape)\n",
    "    data = np.concatenate([x[:,:f_start],mtx],axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_matrix(df,input_sequence_length,target_sequence_length):\n",
    "    ############## get time series data #################\n",
    "    whole = input_sequence_length+target_sequence_length\n",
    "    TS_mean = []\n",
    "    X = df[:whole,0]\n",
    "    X = np.log1p(X)\n",
    "    ts_mean = X.mean()\n",
    "    TS_mean.append(ts_mean)\n",
    "    X = X/ts_mean\n",
    "    Y = df[:input_sequence_length,0]\n",
    "    \n",
    "    feature = df[:whole,1:]\n",
    "    # other_features = df[SR_FEATURE][:whole]\n",
    "    # feature = np.hstack([stable_features,other_features])\n",
    "    #Y_feature = np.hstack([df[ENC_COL][input_sequence_length:],df[SR_FEATURE][input_sequence_length:]])\n",
    "    return X,feature,Y,TS_mean\n",
    "\n",
    "def get_teaching_force(feature_col,encoder_input,decoder_output):\n",
    "    decoder_input_train = np.zeros((decoder_output.shape[0], decoder_output.shape[1],feature_col))\n",
    "    # decoder_output = \n",
    "    # X,feature,Y= get_ts_matrix(df,decoder_output.shape[0], decoder_output.shape[1])\n",
    "    #decode_input_train= np.zeros((20000,18,fea_len))\n",
    "    decoder_input_train[:,1:,:] = decoder_output[:,:-1,:]\n",
    "    decoder_input_train[:,0,:] = encoder_input[:,-1,:]\n",
    "    return decoder_input_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = TRAIN_FEATURE_DATA[10001][:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_LENGTH = get_feature_num(ss)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "batch_size = 128\n",
    "def data_generator(feature_list,sr_list,batch_size, steps_per_epoch,feature_num,\n",
    "                input_sequence_length, target_sequence_length,seed=43,steps=1):\n",
    "    whole_ts_length = input_sequence_length + target_sequence_length\n",
    "    \n",
    "    while True:\n",
    "        np.random.seed(seed)\n",
    "        for _ in range(steps_per_epoch):\n",
    "            # Number of batches of size batch_size produced by generator.\n",
    "            # start_index = 0\n",
    "            data = np.zeros((batch_size,whole_ts_length,feature_num))\n",
    "            \n",
    "            start_index_list = random.sample(range(0,len(feature_list)),batch_size)\n",
    "           \n",
    "            for batch_num in range(batch_size):\n",
    "                # sample_data = df[(df['LC']==lc_list[batch_num])&df['product']==product_list[batch_num]].sort_values(by=['week_begin'],ascending=True)\n",
    "                index_list = random.sample(range(0,len(feature_list)),batch_size)\n",
    "                # batch = df[start_index_list[batch_num]:start_index_list[batch_num]+whole_ts_length]\n",
    "                # print(index_list[batch_num])\n",
    "                x_whole,feature,y,ts_mean_list = get_batch_matrix(feature_list,sr_list,index_list[batch_num],\n",
    "                                                                  ENC_LENGTH,PRED_LENGTH)\n",
    "                # encode_feature,feature_dim = get_encode_mtx(feature[:,:len(ENC_COL)],feature[:,len(ENC_COL):])\n",
    "                # print(feature.shape)\n",
    "            \n",
    "                #print(data[0,:,0])\n",
    "                for i in range(whole_ts_length):\n",
    "                    data[batch_num,i,:] = feature[i]\n",
    "                data[batch_num,:,0] = x_whole\n",
    "                \n",
    "            \n",
    "            encoder_input = data[:,:input_sequence_length,:]\n",
    "            decoder_output = np.expand_dims(data[:,input_sequence_length:,0],axis=2)\n",
    "            '''\n",
    "            The output of the generator must be ([encoder_input, decoder_input], [decoder_output])\n",
    "            add teaching force to prepare data for training\n",
    "            '''\n",
    "            decoder_input = get_teaching_force(feature_num,encoder_input,\n",
    "                                               data[:,input_sequence_length:,:])\n",
    "                                                                        \n",
    "            train_input = np.concatenate([encoder_input, decoder_input], axis=1)\n",
    "            \n",
    "            yield (train_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate,Activation,BatchNormalization,SpatialDropout1D\n",
    "from keras.optimizers import Adam,rmsprop\n",
    "\n",
    "# convolutional layer parameters\n",
    "n_filters = 32 \n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(9)] \n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "val_batch_size = 64\n",
    "steps_per_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an input history series and pass it through a stack of dilated causal convolutions. \n",
    "history_seq = Input(shape=(None, FEATURE_LENGTH))\n",
    "x = history_seq\n",
    "\n",
    "for dilation_rate in dilation_rates:\n",
    "    x = Conv1D(filters=n_filters,\n",
    "               kernel_size=filter_width, \n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "# extract the last 18 time steps as the training target\n",
    "def slice(x, PRED_LENGTH):\n",
    "    return x[:,-PRED_LENGTH:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seq_train = Lambda(slice, arguments={'PRED_LENGTH':18})(x)\n",
    "\n",
    "model = Model(history_seq, pred_seq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 68)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 32)          4384      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 128)         4224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 1)           129       \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, None, 1)           0         \n",
      "=================================================================\n",
      "Total params: 25,377\n",
      "Trainable params: 25,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:999: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "500/500 [==============================] - 131s 263ms/step - loss: 0.9995 - val_loss: 0.2270\n",
      "Epoch 2/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.2191 - val_loss: 0.2167\n",
      "Epoch 3/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.2146 - val_loss: 0.2204\n",
      "Epoch 4/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.2091 - val_loss: 0.1856\n",
      "Epoch 5/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.1611 - val_loss: 0.1478\n",
      "Epoch 6/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.1424 - val_loss: 0.1232\n",
      "Epoch 7/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.1334 - val_loss: 0.1174\n",
      "Epoch 8/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.1280 - val_loss: 0.1459\n",
      "Epoch 9/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.1243 - val_loss: 0.1190\n",
      "Epoch 10/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.1213 - val_loss: 0.0978\n",
      "Epoch 11/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.1198 - val_loss: 0.1096\n",
      "Epoch 12/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.1149 - val_loss: 0.1021\n",
      "Epoch 13/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.1134 - val_loss: 0.0965\n",
      "Epoch 14/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.1120 - val_loss: 0.1017\n",
      "Epoch 15/200\n",
      "120/500 [======>.......................] - ETA: 1:23 - loss: 0.1106"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 125s 251ms/step - loss: 0.1072 - val_loss: 0.0998\n",
      "Epoch 18/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.1086 - val_loss: 0.1065\n",
      "Epoch 19/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.1071 - val_loss: 0.1031\n",
      "Epoch 20/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.1061 - val_loss: 0.1227\n",
      "Epoch 21/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.1041 - val_loss: 0.1073\n",
      "Epoch 22/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.1052 - val_loss: 0.0959\n",
      "Epoch 23/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.1053 - val_loss: 0.0974\n",
      "Epoch 24/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.1039 - val_loss: 0.1130\n",
      "Epoch 25/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.1028 - val_loss: 0.1012\n",
      "Epoch 26/200\n",
      "252/500 [==============>...............] - ETA: 54s - loss: 0.1031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 244ms/step - loss: 0.1023 - val_loss: 0.1118\n",
      "Epoch 29/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.1015 - val_loss: 0.1314\n",
      "Epoch 30/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.1012 - val_loss: 0.1134\n",
      "Epoch 31/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.1014 - val_loss: 0.1044\n",
      "Epoch 32/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.1001 - val_loss: 0.1247\n",
      "Epoch 33/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.1003 - val_loss: 0.1073\n",
      "Epoch 34/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0998 - val_loss: 0.1046\n",
      "Epoch 35/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0991 - val_loss: 0.1012\n",
      "Epoch 36/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.1008 - val_loss: 0.1030\n",
      "Epoch 37/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0995 - val_loss: 0.1432\n",
      "Epoch 38/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0991 - val_loss: 0.1113\n",
      "Epoch 39/200\n",
      "294/500 [================>.............] - ETA: 46s - loss: 0.0994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0990 - val_loss: 0.1140\n",
      "Epoch 42/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0979 - val_loss: 0.1120\n",
      "Epoch 43/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0994 - val_loss: 0.1075\n",
      "Epoch 44/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0990 - val_loss: 0.1144\n",
      "Epoch 45/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0986 - val_loss: 0.1074\n",
      "Epoch 46/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0975 - val_loss: 0.1209\n",
      "Epoch 47/200\n",
      "334/500 [===================>..........] - ETA: 37s - loss: 0.0983"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0975 - val_loss: 0.1081\n",
      "Epoch 50/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0985 - val_loss: 0.1336\n",
      "Epoch 51/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0965 - val_loss: 0.1059\n",
      "Epoch 52/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0976 - val_loss: 0.1119\n",
      "Epoch 53/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0970 - val_loss: 0.1078\n",
      "Epoch 54/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0972 - val_loss: 0.1036\n",
      "Epoch 55/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0977 - val_loss: 0.1056\n",
      "Epoch 56/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0974 - val_loss: 0.1114\n",
      "Epoch 57/200\n",
      "105/500 [=====>........................] - ETA: 1:24 - loss: 0.0955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0962 - val_loss: 0.1068\n",
      "Epoch 60/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0963 - val_loss: 0.1262\n",
      "Epoch 61/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0971 - val_loss: 0.1097\n",
      "Epoch 62/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0977 - val_loss: 0.1102\n",
      "Epoch 63/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0961 - val_loss: 0.1054\n",
      "Epoch 64/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0955 - val_loss: 0.1032\n",
      "Epoch 65/200\n",
      "120/500 [======>.......................] - ETA: 1:21 - loss: 0.0965"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0956 - val_loss: 0.1051\n",
      "Epoch 68/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0960 - val_loss: 0.1085\n",
      "Epoch 69/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0953 - val_loss: 0.1047\n",
      "Epoch 70/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0959 - val_loss: 0.1103\n",
      "Epoch 71/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0950 - val_loss: 0.1078\n",
      "Epoch 72/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0958 - val_loss: 0.1004\n",
      "Epoch 73/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0955 - val_loss: 0.1268\n",
      "Epoch 74/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0962 - val_loss: 0.1284\n",
      "Epoch 75/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0941 - val_loss: 0.1040\n",
      "Epoch 76/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0948 - val_loss: 0.1112\n",
      "Epoch 77/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0946 - val_loss: 0.1199\n",
      "Epoch 78/200\n",
      " 82/500 [===>..........................] - ETA: 1:26 - loss: 0.0931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0949 - val_loss: 0.1093\n",
      "Epoch 81/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0944 - val_loss: 0.1275\n",
      "Epoch 82/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0941 - val_loss: 0.1186\n",
      "Epoch 83/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0941 - val_loss: 0.1101\n",
      "Epoch 84/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0939 - val_loss: 0.1090\n",
      "Epoch 85/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0947 - val_loss: 0.1129\n",
      "Epoch 86/200\n",
      "108/500 [=====>........................] - ETA: 1:24 - loss: 0.0953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0949 - val_loss: 0.1212\n",
      "Epoch 89/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0933 - val_loss: 0.1183\n",
      "Epoch 90/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0945 - val_loss: 0.1273\n",
      "Epoch 91/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0935 - val_loss: 0.1128\n",
      "Epoch 92/200\n",
      "500/500 [==============================] - 128s 255ms/step - loss: 0.0942 - val_loss: 0.1102\n",
      "Epoch 93/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0924 - val_loss: 0.1056\n",
      "Epoch 94/200\n",
      "137/500 [=======>......................] - ETA: 1:18 - loss: 0.0949"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0938 - val_loss: 0.1124\n",
      "Epoch 97/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0945 - val_loss: 0.1074\n",
      "Epoch 98/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0950 - val_loss: 0.1124\n",
      "Epoch 99/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0930 - val_loss: 0.0999\n",
      "Epoch 100/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0933 - val_loss: 0.1065\n",
      "Epoch 101/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0945 - val_loss: 0.1050\n",
      "Epoch 102/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0939 - val_loss: 0.1266\n",
      "Epoch 103/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0930 - val_loss: 0.1199\n",
      "Epoch 104/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0927 - val_loss: 0.1060\n",
      "Epoch 105/200\n",
      "187/500 [==========>...................] - ETA: 1:08 - loss: 0.0941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0928 - val_loss: 0.1058\n",
      "Epoch 108/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0933 - val_loss: 0.1004\n",
      "Epoch 109/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0931 - val_loss: 0.1093\n",
      "Epoch 110/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0929 - val_loss: 0.1174\n",
      "Epoch 111/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0937 - val_loss: 0.1073\n",
      "Epoch 112/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0939 - val_loss: 0.1121\n",
      "Epoch 113/200\n",
      "214/500 [===========>..................] - ETA: 1:04 - loss: 0.0916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0935 - val_loss: 0.1100\n",
      "Epoch 116/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0929 - val_loss: 0.1310\n",
      "Epoch 117/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0937 - val_loss: 0.1038\n",
      "Epoch 118/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0928 - val_loss: 0.1091\n",
      "Epoch 119/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0931 - val_loss: 0.1329\n",
      "Epoch 120/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0919 - val_loss: 0.1120\n",
      "Epoch 121/200\n",
      "246/500 [=============>................] - ETA: 57s - loss: 0.0923"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0929 - val_loss: 0.1210\n",
      "Epoch 124/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0932 - val_loss: 0.0994\n",
      "Epoch 125/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0937 - val_loss: 0.1049\n",
      "Epoch 126/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0938 - val_loss: 0.1211\n",
      "Epoch 127/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0933 - val_loss: 0.1193\n",
      "Epoch 128/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0914 - val_loss: 0.1072\n",
      "Epoch 129/200\n",
      "278/500 [===============>..............] - ETA: 48s - loss: 0.0930"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0928 - val_loss: 0.1183\n",
      "Epoch 132/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0925 - val_loss: 0.1135\n",
      "Epoch 133/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0927 - val_loss: 0.1069\n",
      "Epoch 134/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0923 - val_loss: 0.1224\n",
      "Epoch 135/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0920 - val_loss: 0.1178\n",
      "Epoch 136/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0917 - val_loss: 0.1104\n",
      "Epoch 137/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0918 - val_loss: 0.1139\n",
      "Epoch 138/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0933 - val_loss: 0.1057\n",
      "Epoch 139/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0924 - val_loss: 0.1201\n",
      "Epoch 140/200\n",
      "226/500 [============>.................] - ETA: 1:00 - loss: 0.0920"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0927 - val_loss: 0.1113\n",
      "Epoch 143/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0922 - val_loss: 0.1140\n",
      "Epoch 144/200\n",
      "500/500 [==============================] - 129s 258ms/step - loss: 0.0928 - val_loss: 0.1189\n",
      "Epoch 145/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0928 - val_loss: 0.1079\n",
      "Epoch 146/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0920 - val_loss: 0.1107\n",
      "Epoch 147/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0919 - val_loss: 0.1079\n",
      "Epoch 148/200\n",
      "230/500 [============>.................] - ETA: 1:00 - loss: 0.0927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0921 - val_loss: 0.1165\n",
      "Epoch 151/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0927 - val_loss: 0.1086\n",
      "Epoch 152/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0928 - val_loss: 0.1102\n",
      "Epoch 153/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0923 - val_loss: 0.1064\n",
      "Epoch 154/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0921 - val_loss: 0.1135\n",
      "Epoch 155/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0918 - val_loss: 0.1217\n",
      "Epoch 156/200\n",
      "259/500 [==============>...............] - ETA: 53s - loss: 0.0924"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0916 - val_loss: 0.1106\n",
      "Epoch 159/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0915 - val_loss: 0.1085\n",
      "Epoch 160/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0916 - val_loss: 0.1270\n",
      "Epoch 161/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0926 - val_loss: 0.1151\n",
      "Epoch 162/200\n",
      "500/500 [==============================] - 121s 243ms/step - loss: 0.0932 - val_loss: 0.1205\n",
      "Epoch 163/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0922 - val_loss: 0.1148\n",
      "Epoch 164/200\n",
      "261/500 [==============>...............] - ETA: 53s - loss: 0.0912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0911 - val_loss: 0.1029\n",
      "Epoch 167/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0922 - val_loss: 0.1225\n",
      "Epoch 168/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0919 - val_loss: 0.1176\n",
      "Epoch 169/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0916 - val_loss: 0.1124\n",
      "Epoch 170/200\n",
      "500/500 [==============================] - 127s 255ms/step - loss: 0.0927 - val_loss: 0.1203\n",
      "Epoch 171/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0913 - val_loss: 0.1200\n",
      "Epoch 172/200\n",
      "257/500 [==============>...............] - ETA: 54s - loss: 0.0914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0924 - val_loss: 0.1054\n",
      "Epoch 175/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0915 - val_loss: 0.1354\n",
      "Epoch 176/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0932 - val_loss: 0.1151\n",
      "Epoch 177/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0924 - val_loss: 0.1237\n",
      "Epoch 178/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0916 - val_loss: 0.1232\n",
      "Epoch 179/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0918 - val_loss: 0.1256\n",
      "Epoch 180/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0901 - val_loss: 0.1069\n",
      "Epoch 181/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0916 - val_loss: 0.1121\n",
      "Epoch 182/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0917 - val_loss: 0.1221\n",
      "Epoch 183/200\n",
      "500/500 [==============================] - 121s 242ms/step - loss: 0.0908 - val_loss: 0.1144\n",
      "Epoch 184/200\n",
      " 93/500 [====>.........................] - ETA: 1:24 - loss: 0.0922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0914 - val_loss: 0.1093\n",
      "Epoch 187/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0927 - val_loss: 0.1140\n",
      "Epoch 188/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0921 - val_loss: 0.1059\n",
      "Epoch 189/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0915 - val_loss: 0.1218\n",
      "Epoch 190/200\n",
      "500/500 [==============================] - 121s 242ms/step - loss: 0.0911 - val_loss: 0.1091\n",
      "Epoch 191/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0897 - val_loss: 0.1175\n",
      "Epoch 192/200\n",
      "121/500 [======>.......................] - ETA: 1:21 - loss: 0.0902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0911 - val_loss: 0.1109\n",
      "Epoch 195/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0917 - val_loss: 0.1121\n",
      "Epoch 196/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0917 - val_loss: 0.1341\n",
      "Epoch 197/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0905 - val_loss: 0.1197\n",
      "Epoch 198/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0912 - val_loss: 0.1290\n",
      "Epoch 199/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0915 - val_loss: 0.1095\n",
      "Epoch 200/200\n",
      "114/500 [=====>........................] - ETA: 1:20 - loss: 0.0902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lagging_force\n",
    "# fea_len = 576\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,LearningRateScheduler\n",
    "import keras.backend as kb\n",
    "#import CLR\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=20,\n",
    "                              verbose=1, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20, \n",
    "    verbose=1, mode='auto', epsilon=None)\n",
    "'''\n",
    "# drop-cyclic lr schedule - DIY trainning period\n",
    "def step_decay(epochs):\n",
    "    initial_lr = 0.0005 \n",
    "    period = 15\n",
    "    drop = 0.5\n",
    "    lr = initial_lr*math.pow(drop, math.floor((1+epochs)/period))\n",
    "lr_scheduler = LearningRateScheduler(step_decay, verbose=1)\n",
    "lr_scheduler = CLR.CyclicLR(base_lr=0.0005, max_lr=0.002,\n",
    "                        step_size=200., mode='triangular2')\n",
    "'''\n",
    "callbacks = [earlystopping,lr_reducer]\n",
    "\n",
    "rmsprop = optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None, decay=1e-9)\n",
    "d_model.compile(rmsprop, loss='mean_absolute_error')\n",
    "\n",
    "train_data_generator = data_generator(TRAIN_FEATURE_DATA,TRAIN_SR_DATA,batch_size=batch_size,\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                      feature_num = FEATURE_LENGTH,\n",
    "                                   input_sequence_length=ENC_LENGTH,\n",
    "                                   target_sequence_length=PRED_LENGTH,\n",
    "                                    seed=100)\n",
    "val_data_generator = data_generator(VAL_FEATURE_DATA,VAL_SR_DATA,batch_size=val_batch_size,\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                      feature_num = FEATURE_LENGTH,\n",
    "                                   input_sequence_length=ENC_LENGTH,\n",
    "                                   target_sequence_length=PRED_LENGTH,\n",
    "                                    seed=43)\n",
    "#kb.clear_session()\n",
    "history = d_model.fit_generator(train_data_generator, \n",
    "                    steps_per_epoch = steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_data_generator,\n",
    "                             validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f512f5ce470>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend(['Train','Valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model.save('./base03.h5')\n",
    "d_model.save_weights('./base03_w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "dd = models.load_model('./base02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FEATURE_DATA,TEST_SR_DATA = get_ts_info(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_size=64\n",
    "steps_per_epoch = 500\n",
    "test_data_generator = data_generator(TEST_FEATURE_DATA,TEST_SR_DATA,batch_size=val_batch_size,\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                      feature_num = FEATURE_LENGTH,\n",
    "                                   input_sequence_length=ENC_LENGTH,\n",
    "                                   target_sequence_length=PRED_LENGTH,\n",
    "                                    seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 220s 110ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07494487369246781"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.evaluate_generator(test_data_generator,steps=2000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(my_model,datagen,pred_steps=18,sample_size=batch_size):\n",
    "    pred_seq = np.zeros((sample_size,pred_steps,1))\n",
    "    iterator = iter(datagen)\n",
    "    input_sequence = next(iterator)\n",
    "    target = input_sequence[1]\n",
    "    x = input_sequence[0]\n",
    "    for j in range(pred_steps):\n",
    "        last_pred_step = my_model.predict(x)[:,-1,0]\n",
    "        #print(last_pred_step.shape)\n",
    "        pred_seq[:,j,0] = last_pred_step\n",
    "        #print(input_sequence[0].shape)\n",
    "        last_step = np.zeros((sample_size,1,FEATURE_LENGTH))\n",
    "        last_step[:,0,:] = input_sequence[0][:,ENC_LENGTH+j,:]\n",
    "        last_step[:,0,0] = last_pred_step\n",
    "        input_ts = np.concatenate([input_sequence[0],last_step],axis=1)\n",
    "        #print(input_ts.shape)\n",
    "        x = input_ts\n",
    "    return pred_seq,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = data_generator(TEST_FEATURE_DATA[-1000:],TEST_SR_DATA[-1000:],batch_size=1000,feature_num = FEATURE_LENGTH,\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                                   input_sequence_length=ENC_LENGTH,\n",
    "                                   target_sequence_length=PRED_LENGTH,\n",
    "                                    seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict,target = predict_sequence(dd,test_gen,sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Results Analysis ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss_summary(target,test,sample_size):\n",
    "    single_spot_mape = []\n",
    "    single_ts_mape= []\n",
    "    for i in range(sample_size):\n",
    "        ans,single_loss = cal_sample_weighted_loss(target,test,i)\n",
    "        single_spot_mape.append(ans)\n",
    "        single_ts_mape.append(single_loss)\n",
    "    return single_spot_mape,single_ts_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sample_weighted_loss(target,pred,sample_ind):\n",
    "    y_real = target[sample_ind,:,0].reshape(-1,1)\n",
    "    y_pred = pred.reshape(-1,1)\n",
    "    ans = list(map(lambda x: abs(x[0]-x[1])/x[0],zip(y_real,y_pred)))\n",
    "    #loss = (y_real-y_pred)/y_real\n",
    "    loss_w = list(map(lambda x: abs(x[0]-x[1])*abs(x[0]),zip(y_real,y_pred)))\n",
    "    loss_sr = list(map(lambda x: x[0]**2, y_real))\n",
    "    loss_sum = sum(loss_w)/sum(loss_sr)\n",
    "    return ans,loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_mape,weighted_mape = cal_sample_weighted_loss(target,predict,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_spot_mape,single_ts_mape = weighted_loss_summary(target,predict,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spot_mape = np.mean(single_spot_mape)\n",
    "max_spot_mape = np.max(single_spot_mape)\n",
    "min_spot_mape = np.min(single_spot_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_weighted_mape = pd.DataFrame(single_spot_mape,columns=range(18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_mape_array = np.array(single_spot_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ts_mape_array.reshape((1000,18))\n",
    "xmean = x.mean(axis=1)\n",
    "xmax = x.max(axis=1)\n",
    "xmin = x.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_info = np.vstack((xmean,xmax))\n",
    "mape_info = np.vstack((mape_info,xmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mape_info.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_frame = pd.DataFrame(m,columns=['mean','max','min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.325018</td>\n",
       "      <td>0.506290</td>\n",
       "      <td>0.157907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.020810</td>\n",
       "      <td>1.555811</td>\n",
       "      <td>0.576609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.013527</td>\n",
       "      <td>0.027927</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.042585</td>\n",
       "      <td>0.087391</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.065293</td>\n",
       "      <td>0.148106</td>\n",
       "      <td>0.005728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.238492</td>\n",
       "      <td>0.419762</td>\n",
       "      <td>0.059909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.118881</td>\n",
       "      <td>25.061394</td>\n",
       "      <td>6.106443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean          max          min\n",
       "count  1000.000000  1000.000000  1000.000000\n",
       "mean      0.325018     0.506290     0.157907\n",
       "std       1.020810     1.555811     0.576609\n",
       "min       0.013527     0.027927     0.000003\n",
       "25%       0.042585     0.087391     0.001783\n",
       "50%       0.065293     0.148106     0.005728\n",
       "75%       0.238492     0.419762     0.059909\n",
       "max      14.118881    25.061394     6.106443"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_frame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(feature_list, sr_list, sample_ind, my_model,enc_tail_len=72,pred_steps=18):\n",
    "    input_series = get_batch_input(feature_list, sr_list, sample_ind, enc_tail_len, pred_steps)\n",
    "    pred_series = my_model.predict(input_series)\n",
    "    \n",
    "    #input_series = input_series.reshape(-1,1)\n",
    "    pred_series = pred_series.reshape(-1,1)   \n",
    "    target_series = sr_list[sample_ind][enc_tail_len:].reshape(-1,1)\n",
    "    # print(target_series.shape)\n",
    "    pred = (np.mean(sr_list[sample_ind]))*pred_series\n",
    "    \n",
    "    encode_series_tail = sr_list[sample_ind][:72].reshape(-1,1)\n",
    "    # print(encode_series_tail.shape)\n",
    "    x_encode = enc_tail_len\n",
    "    \n",
    "    plt.figure(figsize=(10,6))   \n",
    "    \n",
    "    plt.plot(range(1,x_encode+1),encode_series_tail)\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),pred,color='teal',linestyle='--')\n",
    "    \n",
    "    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n",
    "    plt.legend(['Encoding Series','Target Series','Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_input(feature_list, sr_list, sample_ind, input_seq_length, pred_length):\n",
    "    s = np.zeros((1,whole_ts,FEATURE_LENGTH))\n",
    "    x,feature,y,ts_mean_list = get_batch_matrix(feature_list,sr_list,sample_ind,ENC_LENGTH,PRED_LENGTH)\n",
    "    for i in range(whole_ts):\n",
    "        s[0,i,:] = feature[i]\n",
    "    s[0,:,0] = x\n",
    "    enc_input = s[:,:ENC_LENGTH,:]\n",
    "    dec_output = np.expand_dims(s[:,ENC_LENGTH:,0],axis=2)\n",
    "    dec_input = get_teaching_force(FEATURE_LENGTH,enc_input,s[:,ENC_LENGTH:,:])\n",
    "    in_data = np.concatenate([enc_input, dec_input], axis=1)\n",
    "    return in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_and_plot(TEST_FEATURE_DATA, TEST_SR_DATA,670, dd, enc_tail_len=72,pred_steps=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../base1.h5')\n",
    "model.save_weights('../base1_w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "t_model = models.load_model('../base1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## CNN+dropout/LSTM #####################\n",
    "'''\n",
    "seems to be easily overfitting--high val loss and probably skip-out\n",
    "1st solution: CNN+spatialdropout\n",
    "2st solution: LSTM wrapped--(attemp to learn better?)\n",
    "'''\n",
    "# convolutional layer parameters\n",
    "from keras.layers import SpatialDropout1D\n",
    "n_filters = 32 \n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(8)] \n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "val_batch_size = 64\n",
    "steps_per_epoch = 500\n",
    "\n",
    "# define an input history series and pass it through a stack of dilated causal convolutions. \n",
    "history_seq = Input(shape=(None, FEATURE_LENGTH))\n",
    "x = history_seq\n",
    "dropout_rate = .2\n",
    "name = ''\n",
    "\n",
    "for dilation_rate in dilation_rates:\n",
    "    x = Conv1D(filters=n_filters,\n",
    "               kernel_size=filter_width, \n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate)(x)\n",
    "    x = SpatialDropout1D(dropout_rate, name=name + 'spatial_dropout1d_%d_%f' % (dilation_rate*2,dropout_rate))(x)\n",
    "    \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.2)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "# extract the last 18 time steps as the training target\n",
    "def slice(x, PRED_LENGTH):\n",
    "    return x[:,-PRED_LENGTH:,:]\n",
    "\n",
    "pred_seq_train = Lambda(slice, arguments={'PRED_LENGTH':18})(x)\n",
    "\n",
    "d_model = Model(history_seq, pred_seq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_39 (InputLayer)        (None, None, 68)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, None, 32)          4384      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2_0.200000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4_0.200000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8_0.200000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_16_0.20000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_32_0.20000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_90 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_64_0.20000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_91 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_128_0.2000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_256_0.2000 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, None, 128)         4224      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, None, 1)           129       \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, None, 1)           0         \n",
      "=================================================================\n",
      "Total params: 23,297\n",
      "Trainable params: 23,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Residual Network Added ##################\n",
    "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0,name=''):\n",
    "    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n",
    "    original_x = x\n",
    "    # print('input size-----',original_x.get_shape())\n",
    "    x = BatchNormalization(axis=-1,name='bn_layer_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
    "    x = Activation('linear')(x)\n",
    "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding=padding)(x)\n",
    "    x = Activation('linear')(x)    \n",
    "    x = SpatialDropout1D(dropout_rate, name=name + 'spatial_dropout1d_%d_s%d_%f' % (i*2, s, dropout_rate))(x)\n",
    "    # print('-----------------2dilate',x.get_shape())\n",
    "    short_cut = Conv1D(nb_filters, 1, padding='same')(x)\n",
    "    # 1x1 conv.\n",
    "    x = Conv1D(nb_filters, 1, padding='same')(x)\n",
    "    residual_x =add([original_x, x])\n",
    "    return residual_x,short_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "############tcn model#################\n",
    "# convolutional layer parameters\n",
    "n_filters = 32 \n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(8)] \n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "val_batch_size = 64\n",
    "steps_per_epoch = 500\n",
    "PRED_LENGTH = 18\n",
    "\n",
    "def my_tcn_model(dilation_rate_range,nb_filters,inputs,padding,n_stacks,kernel_size,\n",
    "                 dropout_rate,activation,\n",
    "                 use_skip_connections=True,return_sequences=True):\n",
    "    # define an input history series and pass it through a stack of dilated causal convolutions. \n",
    "    \n",
    "    x = inputs\n",
    "    x = Conv1D(filters=nb_filters, padding=padding,dilation_rate=1,kernel_size=filter_width)(x)\n",
    "    S = []\n",
    "    for s in range(n_stacks):\n",
    "        for i in dilation_rate_range:\n",
    "            x, short_cut = residual_block(x, s, i, activation, n_filters,kernel_size, padding, dropout_rate,activation)\n",
    "            # print(x.get_shape())\n",
    "            S.append(short_cut)\n",
    "    if use_skip_connections:\n",
    "        skip_connection = add(S)\n",
    "        x = skip_connection\n",
    "        \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(.1)(x)\n",
    "    x = Dense(1)(x)\n",
    "\n",
    "    if not return_sequences:\n",
    "        output_slice_index = -1\n",
    "        x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
    "    else:\n",
    "        x = Lambda(lambda tt: tt[:, -PRED_LENGTH:, :])(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate,SpatialDropout1D,add\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "history_seq = Input(shape=(None, FEATURE_LENGTH))\n",
    "x = history_seq\n",
    "\n",
    "pred_seq_target = my_tcn_model(dilation_rates,n_filters,x,'causal',1,filter_width,.1,'relu')\n",
    "#pred_no_skip = my_tcn_model(dilation_rates,n_filters,x,'causal',3,filter_width,.1,'relu',use_skip_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model = Model(history_seq,pred_seq_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, None, 68)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, None, 32)     4384        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_1_s0_0.100000 (BatchNo (None, None, 32)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 32)     0           bn_layer_1_s0_0.100000[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_2_s0_0.10 (None, None, 32)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_2_s0_0.1000\n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 32)     0           conv1d_46[0][0]                  \n",
      "                                                                 conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_2_s0_0.100000 (BatchNo (None, None, 32)     128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 32)     0           bn_layer_2_s0_0.100000[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_4_s0_0.10 (None, None, 32)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_4_s0_0.1000\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, 32)     0           add_1[0][0]                      \n",
      "                                                                 conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_4_s0_0.100000 (BatchNo (None, None, 32)     128         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, 32)     0           bn_layer_4_s0_0.100000[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, 32)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_8_s0_0.10 (None, None, 32)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_8_s0_0.1000\n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, 32)     0           add_2[0][0]                      \n",
      "                                                                 conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_8_s0_0.100000 (BatchNo (None, None, 32)     128         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 32)     0           bn_layer_8_s0_0.100000[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 32)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_16_s0_0.1 (None, None, 32)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_16_s0_0.100\n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, 32)     0           add_3[0][0]                      \n",
      "                                                                 conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_16_s0_0.100000 (BatchN (None, None, 32)     128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 32)     0           bn_layer_16_s0_0.100000[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 32)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_32_s0_0.1 (None, None, 32)     0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_32_s0_0.100\n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, 32)     0           add_4[0][0]                      \n",
      "                                                                 conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_32_s0_0.100000 (BatchN (None, None, 32)     128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, 32)     0           bn_layer_32_s0_0.100000[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, 32)     0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_64_s0_0.1 (None, None, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_64_s0_0.100\n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, 32)     0           add_5[0][0]                      \n",
      "                                                                 conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_64_s0_0.100000 (BatchN (None, None, 32)     128         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, 32)     0           bn_layer_64_s0_0.100000[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_128_s0_0. (None, None, 32)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_128_s0_0.10\n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, 32)     0           add_6[0][0]                      \n",
      "                                                                 conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_layer_128_s0_0.100000 (Batch (None, None, 32)     128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 32)     0           bn_layer_128_s0_0.100000[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 32)     0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reluspatial_dropout1d_256_s0_0. (None, None, 32)     0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_2_s0_0.1000\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_4_s0_0.1000\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_8_s0_0.1000\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_16_s0_0.100\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_32_s0_0.100\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_64_s0_0.100\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_128_s0_0.10\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, None, 32)     1056        reluspatial_dropout1d_256_s0_0.10\n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, 32)     0           conv1d_48[0][0]                  \n",
      "                                                                 conv1d_51[0][0]                  \n",
      "                                                                 conv1d_54[0][0]                  \n",
      "                                                                 conv1d_57[0][0]                  \n",
      "                                                                 conv1d_60[0][0]                  \n",
      "                                                                 conv1d_63[0][0]                  \n",
      "                                                                 conv1d_66[0][0]                  \n",
      "                                                                 conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, None, 128)    4224        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, 128)    0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, None, 1)      129         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None, 1)      0           dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,601\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "r_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "500/500 [==============================] - 130s 261ms/step - loss: 0.3571 - val_loss: 0.2048\n",
      "Epoch 2/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.2171 - val_loss: 0.1662\n",
      "Epoch 3/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.1579 - val_loss: 0.1201\n",
      "Epoch 4/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.1375 - val_loss: 0.1378\n",
      "Epoch 5/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.1256 - val_loss: 0.1237\n",
      "Epoch 6/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.1172 - val_loss: 0.1152\n",
      "Epoch 7/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.1122 - val_loss: 0.1010\n",
      "Epoch 8/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.1061 - val_loss: 0.0991\n",
      "Epoch 9/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.1026 - val_loss: 0.1087\n",
      "Epoch 10/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0982 - val_loss: 0.1059\n",
      "Epoch 11/200\n",
      "500/500 [==============================] - 128s 255ms/step - loss: 0.0962 - val_loss: 0.1012\n",
      "Epoch 12/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0940 - val_loss: 0.1108\n",
      "Epoch 13/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0918 - val_loss: 0.1079\n",
      "Epoch 14/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0893 - val_loss: 0.1085\n",
      "Epoch 15/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0884 - val_loss: 0.0982\n",
      "Epoch 16/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0871 - val_loss: 0.0908\n",
      "Epoch 17/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0849 - val_loss: 0.0992\n",
      "Epoch 18/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0839 - val_loss: 0.0967\n",
      "Epoch 19/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0830 - val_loss: 0.0826\n",
      "Epoch 20/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0823 - val_loss: 0.0986\n",
      "Epoch 21/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0804 - val_loss: 0.1032\n",
      "Epoch 22/200\n",
      "500/500 [==============================] - 127s 255ms/step - loss: 0.0800 - val_loss: 0.0984\n",
      "Epoch 23/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0793 - val_loss: 0.0994\n",
      "Epoch 24/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0795 - val_loss: 0.1027\n",
      "Epoch 25/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0784 - val_loss: 0.1012\n",
      "Epoch 26/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0776 - val_loss: 0.0860\n",
      "Epoch 27/200\n",
      "500/500 [==============================] - 128s 256ms/step - loss: 0.0757 - val_loss: 0.0859\n",
      "Epoch 28/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0761 - val_loss: 0.1012\n",
      "Epoch 29/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0751 - val_loss: 0.1068\n",
      "Epoch 30/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0735 - val_loss: 0.1002\n",
      "Epoch 31/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0730 - val_loss: 0.0917\n",
      "Epoch 32/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0714 - val_loss: 0.0927\n",
      "Epoch 33/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0713 - val_loss: 0.0892\n",
      "Epoch 34/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0707 - val_loss: 0.1035\n",
      "Epoch 35/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0693 - val_loss: 0.0967\n",
      "Epoch 36/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0693 - val_loss: 0.1059\n",
      "Epoch 37/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0682 - val_loss: 0.0989\n",
      "Epoch 38/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0679 - val_loss: 0.0964\n",
      "Epoch 39/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0673 - val_loss: 0.0958\n",
      "Epoch 40/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0667 - val_loss: 0.0982\n",
      "Epoch 41/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0667 - val_loss: 0.1119\n",
      "Epoch 42/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0654 - val_loss: 0.1063\n",
      "Epoch 43/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0653 - val_loss: 0.0838\n",
      "Epoch 44/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0657 - val_loss: 0.0861\n",
      "Epoch 45/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0655 - val_loss: 0.0915\n",
      "Epoch 46/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0642 - val_loss: 0.0954\n",
      "Epoch 47/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0636 - val_loss: 0.0930\n",
      "Epoch 48/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0643 - val_loss: 0.0942\n",
      "Epoch 49/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0634 - val_loss: 0.0949\n",
      "Epoch 50/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0630 - val_loss: 0.0972\n",
      "Epoch 51/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0624 - val_loss: 0.0953\n",
      "Epoch 52/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0633 - val_loss: 0.0971\n",
      "Epoch 53/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0625 - val_loss: 0.0945\n",
      "Epoch 54/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0617 - val_loss: 0.0934\n",
      "Epoch 55/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0618 - val_loss: 0.0861\n",
      "Epoch 56/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0619 - val_loss: 0.0956\n",
      "Epoch 57/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0619 - val_loss: 0.0843\n",
      "Epoch 58/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0617 - val_loss: 0.0889\n",
      "Epoch 59/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0615 - val_loss: 0.0940\n",
      "Epoch 60/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0614 - val_loss: 0.0816\n",
      "Epoch 61/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0619 - val_loss: 0.1016\n",
      "Epoch 62/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0614 - val_loss: 0.0988\n",
      "Epoch 63/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0610 - val_loss: 0.0879\n",
      "Epoch 64/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0607 - val_loss: 0.0831\n",
      "Epoch 65/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0609 - val_loss: 0.0988\n",
      "Epoch 66/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0613 - val_loss: 0.0994\n",
      "Epoch 67/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0597 - val_loss: 0.0904\n",
      "Epoch 68/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0607 - val_loss: 0.0837\n",
      "Epoch 69/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0603 - val_loss: 0.0991\n",
      "Epoch 70/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0607 - val_loss: 0.0845\n",
      "Epoch 71/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0604 - val_loss: 0.0941\n",
      "Epoch 72/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0604 - val_loss: 0.0904\n",
      "Epoch 73/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0598 - val_loss: 0.1032\n",
      "Epoch 74/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0603 - val_loss: 0.0982\n",
      "Epoch 75/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0602 - val_loss: 0.1000\n",
      "Epoch 76/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0603 - val_loss: 0.1014\n",
      "Epoch 77/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0598 - val_loss: 0.1000\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0596 - val_loss: 0.0908\n",
      "Epoch 79/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0595 - val_loss: 0.1006\n",
      "Epoch 80/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0603 - val_loss: 0.0997\n",
      "Epoch 81/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0593 - val_loss: 0.1168\n",
      "Epoch 82/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0596 - val_loss: 0.1074\n",
      "Epoch 83/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0593 - val_loss: 0.1023\n",
      "Epoch 84/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0593 - val_loss: 0.0981\n",
      "Epoch 85/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0594 - val_loss: 0.1071\n",
      "Epoch 86/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0592 - val_loss: 0.1016\n",
      "Epoch 87/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0587 - val_loss: 0.1093\n",
      "Epoch 88/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0596 - val_loss: 0.0909\n",
      "Epoch 89/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0578 - val_loss: 0.0976\n",
      "Epoch 90/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0579 - val_loss: 0.0968\n",
      "Epoch 91/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0588 - val_loss: 0.1091\n",
      "Epoch 92/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0578 - val_loss: 0.1015\n",
      "Epoch 93/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0586 - val_loss: 0.1020\n",
      "Epoch 94/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0585 - val_loss: 0.0982\n",
      "Epoch 95/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0578 - val_loss: 0.1009\n",
      "Epoch 96/200\n",
      "500/500 [==============================] - 121s 243ms/step - loss: 0.0581 - val_loss: 0.0980\n",
      "Epoch 97/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0584 - val_loss: 0.1074\n",
      "Epoch 98/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0589 - val_loss: 0.1049\n",
      "Epoch 99/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0580 - val_loss: 0.1031\n",
      "Epoch 100/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0585 - val_loss: 0.1021\n",
      "Epoch 101/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0584 - val_loss: 0.1023\n",
      "Epoch 102/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0580 - val_loss: 0.1062\n",
      "Epoch 103/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0581 - val_loss: 0.1196\n",
      "Epoch 104/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0577 - val_loss: 0.1073\n",
      "Epoch 105/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0583 - val_loss: 0.1116\n",
      "Epoch 106/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0579 - val_loss: 0.1089\n",
      "Epoch 107/200\n",
      "500/500 [==============================] - 128s 256ms/step - loss: 0.0578 - val_loss: 0.1059\n",
      "Epoch 108/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0574 - val_loss: 0.1028\n",
      "Epoch 109/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0579 - val_loss: 0.1092\n",
      "Epoch 110/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0581 - val_loss: 0.1057\n",
      "Epoch 111/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0572 - val_loss: 0.1069\n",
      "Epoch 112/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0575 - val_loss: 0.1139\n",
      "Epoch 113/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0577 - val_loss: 0.1238\n",
      "Epoch 114/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0573 - val_loss: 0.1031\n",
      "Epoch 115/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0572 - val_loss: 0.1147\n",
      "Epoch 116/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0570 - val_loss: 0.1140\n",
      "Epoch 117/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0577 - val_loss: 0.1058\n",
      "Epoch 118/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0574 - val_loss: 0.1179\n",
      "Epoch 119/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0571 - val_loss: 0.1058\n",
      "Epoch 120/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0570 - val_loss: 0.1099\n",
      "Epoch 121/200\n",
      "500/500 [==============================] - 122s 243ms/step - loss: 0.0579 - val_loss: 0.1103\n",
      "Epoch 122/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0571 - val_loss: 0.1080\n",
      "Epoch 123/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0570 - val_loss: 0.1123\n",
      "Epoch 124/200\n",
      "500/500 [==============================] - 122s 245ms/step - loss: 0.0566 - val_loss: 0.1112\n",
      "Epoch 125/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0565 - val_loss: 0.1252\n",
      "Epoch 126/200\n",
      "500/500 [==============================] - 121s 241ms/step - loss: 0.0573 - val_loss: 0.1123\n",
      "Epoch 127/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0561 - val_loss: 0.1139\n",
      "Epoch 128/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0570 - val_loss: 0.1114\n",
      "Epoch 129/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0569 - val_loss: 0.1160\n",
      "Epoch 130/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0570 - val_loss: 0.1168\n",
      "Epoch 131/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0565 - val_loss: 0.1016\n",
      "Epoch 132/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0571 - val_loss: 0.1149\n",
      "Epoch 133/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0565 - val_loss: 0.1127\n",
      "Epoch 134/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0564 - val_loss: 0.1243\n",
      "Epoch 135/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0557 - val_loss: 0.1172\n",
      "Epoch 136/200\n",
      "500/500 [==============================] - 123s 246ms/step - loss: 0.0560 - val_loss: 0.1168\n",
      "Epoch 137/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0566 - val_loss: 0.1104\n",
      "Epoch 138/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0564 - val_loss: 0.1101\n",
      "Epoch 139/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0566 - val_loss: 0.1183\n",
      "Epoch 140/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0564 - val_loss: 0.1190\n",
      "Epoch 141/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0566 - val_loss: 0.1195\n",
      "Epoch 142/200\n",
      "500/500 [==============================] - 127s 254ms/step - loss: 0.0557 - val_loss: 0.1161\n",
      "Epoch 143/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0561 - val_loss: 0.1219\n",
      "Epoch 144/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0558 - val_loss: 0.1114\n",
      "Epoch 145/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0558 - val_loss: 0.1196\n",
      "Epoch 146/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0561 - val_loss: 0.1184\n",
      "Epoch 147/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0570 - val_loss: 0.1139\n",
      "Epoch 148/200\n",
      "500/500 [==============================] - 124s 248ms/step - loss: 0.0560 - val_loss: 0.1170\n",
      "Epoch 149/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0554 - val_loss: 0.1187\n",
      "Epoch 150/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0563 - val_loss: 0.1159\n",
      "Epoch 151/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0562 - val_loss: 0.1129\n",
      "Epoch 152/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0566 - val_loss: 0.1160\n",
      "Epoch 153/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0562 - val_loss: 0.1289\n",
      "Epoch 154/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0562 - val_loss: 0.1254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0562 - val_loss: 0.1193\n",
      "Epoch 156/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0554 - val_loss: 0.1304\n",
      "Epoch 157/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0557 - val_loss: 0.1194\n",
      "Epoch 158/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0561 - val_loss: 0.1225\n",
      "Epoch 159/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0560 - val_loss: 0.1269\n",
      "Epoch 160/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0554 - val_loss: 0.1188\n",
      "Epoch 161/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0551 - val_loss: 0.1310\n",
      "Epoch 162/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0553 - val_loss: 0.1283\n",
      "Epoch 163/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0553 - val_loss: 0.1234\n",
      "Epoch 164/200\n",
      "500/500 [==============================] - 127s 253ms/step - loss: 0.0557 - val_loss: 0.1295\n",
      "Epoch 165/200\n",
      "500/500 [==============================] - 128s 256ms/step - loss: 0.0555 - val_loss: 0.1262\n",
      "Epoch 166/200\n",
      "500/500 [==============================] - 130s 260ms/step - loss: 0.0552 - val_loss: 0.1280\n",
      "Epoch 167/200\n",
      "500/500 [==============================] - 128s 255ms/step - loss: 0.0557 - val_loss: 0.1173\n",
      "Epoch 168/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0555 - val_loss: 0.1220\n",
      "Epoch 169/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0555 - val_loss: 0.1234\n",
      "Epoch 170/200\n",
      "500/500 [==============================] - 126s 253ms/step - loss: 0.0555 - val_loss: 0.1331\n",
      "Epoch 171/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0551 - val_loss: 0.1245\n",
      "Epoch 172/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0549 - val_loss: 0.1326\n",
      "Epoch 173/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0554 - val_loss: 0.1364\n",
      "Epoch 174/200\n",
      "500/500 [==============================] - 127s 255ms/step - loss: 0.0550 - val_loss: 0.1276\n",
      "Epoch 175/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0555 - val_loss: 0.1301\n",
      "Epoch 176/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0552 - val_loss: 0.1239\n",
      "Epoch 177/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0548 - val_loss: 0.1377\n",
      "Epoch 178/200\n",
      "500/500 [==============================] - 125s 251ms/step - loss: 0.0554 - val_loss: 0.1273\n",
      "Epoch 179/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0554 - val_loss: 0.1346\n",
      "Epoch 180/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0547 - val_loss: 0.1217\n",
      "Epoch 181/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0554 - val_loss: 0.1176\n",
      "Epoch 182/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0553 - val_loss: 0.1205\n",
      "Epoch 183/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0551 - val_loss: 0.1214\n",
      "Epoch 184/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0549 - val_loss: 0.1360\n",
      "Epoch 185/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0552 - val_loss: 0.1335\n",
      "Epoch 186/200\n",
      "500/500 [==============================] - 124s 249ms/step - loss: 0.0552 - val_loss: 0.1268\n",
      "Epoch 187/200\n",
      "500/500 [==============================] - 125s 249ms/step - loss: 0.0550 - val_loss: 0.1339\n",
      "Epoch 188/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0546 - val_loss: 0.1409\n",
      "Epoch 189/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0556 - val_loss: 0.1289\n",
      "Epoch 190/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0551 - val_loss: 0.1412\n",
      "Epoch 191/200\n",
      "500/500 [==============================] - 126s 252ms/step - loss: 0.0549 - val_loss: 0.1344\n",
      "Epoch 192/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0546 - val_loss: 0.1336\n",
      "Epoch 193/200\n",
      "500/500 [==============================] - 123s 247ms/step - loss: 0.0549 - val_loss: 0.1282\n",
      "Epoch 194/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0548 - val_loss: 0.1509\n",
      "Epoch 195/200\n",
      "500/500 [==============================] - 124s 247ms/step - loss: 0.0544 - val_loss: 0.1278\n",
      "Epoch 196/200\n",
      "500/500 [==============================] - 122s 244ms/step - loss: 0.0546 - val_loss: 0.1294\n",
      "Epoch 197/200\n",
      "500/500 [==============================] - 123s 245ms/step - loss: 0.0546 - val_loss: 0.1327\n",
      "Epoch 198/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0548 - val_loss: 0.1333\n",
      "Epoch 199/200\n",
      "500/500 [==============================] - 126s 251ms/step - loss: 0.0549 - val_loss: 0.1292\n",
      "Epoch 200/200\n",
      "500/500 [==============================] - 125s 250ms/step - loss: 0.0545 - val_loss: 0.1236\n"
     ]
    }
   ],
   "source": [
    "r_model.compile(rmsprop, loss='mean_absolute_error')\n",
    "\n",
    "history = r_model.fit_generator(train_data_generator, \n",
    "                    steps_per_epoch = steps_per_epoch,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_data_generator,\n",
    "                             validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model.save('./base04.h5')\n",
    "r_model.save_weights('./base04_w.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 221s 111ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0750373397283256"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_model.evaluate_generator(test_data_generator,steps=2000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
